{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Adam-Aber/Text-Generation-with-RNNs/blob/main/Labs/Lab4/Lab4.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Assignment: Text Generation with RNNs\n",
        "\n",
        "In this assignment, you will explore text generation using deep learning models, specifically Recurrent Neural Networks (RNNs). Text generation is a fascinating task where the goal is to train a model that can predict the next word in a sequence, ultimately generating coherent and contextually accurate sentences or paragraphs.\n",
        "\n",
        "You will work with a corpus of text data provided in the tutorial (from TensorFlow), which contains a large collection of Shakespeare’s works. Your objective is to implement a text generation model using RNNs and to evaluate its performance based on various metrics.\n",
        "\n",
        "<img src=\"https://drek4537l1klr.cloudfront.net/teofili2/Figures/03fig09_alt.jpg\" alt=\"Drawing\"/>\n",
        "\n",
        "**The models include:**\n",
        "- Deep RNN (LSTM)\n",
        "- Deep RNN (GRU)\n",
        "- Bidirectional RNN\n",
        "\n",
        "\n",
        "**Evaluation:**\n",
        "\n",
        "Evaluate the performance of the text generation model using several metrics:\n",
        "- Perplexity: A measure of how well the probability distribution predicted by the model aligns with the actual distribution of words in the text.\n",
        "- Generated Text Quality: Subjectively evaluate the quality of the generated text by considering grammar, coherence, and creativity. This can be done by visually inspecting the generated sequences.\n",
        "- BLUE Score: Character-level BLEU: BLEU can be applied at the character level by treating each character as an n-gram. This is particularly useful when the task involves generating character sequences (like poetry, code, or fine-grained character-based generation).\n",
        "For example, the sequence “hello” could be evaluated by comparing 1-grams (e.g., “h”, “e”, “l”, “l”, “o”) or higher-order n-grams (e.g., “he”, “el”, “ll”, “lo”).\n",
        "\n",
        "**Goals:**\n",
        "\n",
        "- Compare the performance of GRU and LSTM. Is there a significant difference in the results?\n",
        "- Compare the performance of the unidirectional and bidirectional RNN models.\n",
        "Which model produces better results?\n",
        "- Discuss the impact of bidirectional RNNs on text generation tasks."
      ],
      "metadata": {
        "id": "UR9ndGEKbi7G"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Import TensorFlow and other libraries"
      ],
      "metadata": {
        "id": "zWHyOoc1kRzR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import nltk\n",
        "from nltk.translate.bleu_score import sentence_bleu\n",
        "import os\n",
        "import time"
      ],
      "metadata": {
        "id": "RvnvzfT8kU2b"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Dataset"
      ],
      "metadata": {
        "id": "RVCWJVSUkBi_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The Shakespeare dataset used for text generation contains a collection of works by William Shakespeare, primarily in the form of plays and sonnets. The text is used to train language models, offering an ideal example for character-level modeling due to its rich, complex language. The dataset is often preprocessed by tokenizing it into individual characters, enabling models to learn the sequential relationships between characters for generating coherent and contextually appropriate text.\n",
        "\n",
        "To access the dataset, you can use TensorFlow's get_file method as follow:\n",
        "\n",
        "You can inspect the dataset on [Kaggle](https://www.kaggle.com/datasets/adarshpathak/shakespeare-text/data)."
      ],
      "metadata": {
        "id": "w-EJ5MXEkFOF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "path_to_file = tf.keras.utils.get_file('shakespeare.txt', 'https://storage.googleapis.com/download.tensorflow.org/data/shakespeare.txt')\n",
        "\n",
        "# Read, then decode for py2 compat.\n",
        "text = open(path_to_file, 'rb').read().decode(encoding='utf-8')\n",
        "# length of text is the number of characters in it\n",
        "print(f'Length of text: {len(text)} characters')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XQoXSAoflWB9",
        "outputId": "c43e1fd1-9710-4b5c-9605-55d65bba4189"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/download.tensorflow.org/data/shakespeare.txt\n",
            "\u001b[1m1115394/1115394\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 0us/step\n",
            "Length of text: 1115394 characters\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Take a look at the first 250 characters in text\n",
        "print(text[:250])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nbs11e97l08V",
        "outputId": "18482125-d545-4e78-87dc-5daed1959ab9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "First Citizen:\n",
            "Before we proceed any further, hear me speak.\n",
            "\n",
            "All:\n",
            "Speak, speak.\n",
            "\n",
            "First Citizen:\n",
            "You are all resolved rather to die than to famish?\n",
            "\n",
            "All:\n",
            "Resolved. resolved.\n",
            "\n",
            "First Citizen:\n",
            "First, you know Caius Marcius is chief enemy to the people.\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Process the text"
      ],
      "metadata": {
        "id": "KaGt1DN6mhB6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Vectorize the text\n",
        "\n",
        "Before training, you need to convert the strings to a numerical representation.\n",
        "\n",
        "The `tf.keras.layers.StringLookup` layer can convert each character into a numeric ID. It just needs the text to be split into tokens first."
      ],
      "metadata": {
        "id": "WmZYdXLqwgCu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "example_texts = ['abcdefg', 'xyz']\n",
        "\n",
        "chars = tf.strings.unicode_split(example_texts, input_encoding='UTF-8')\n",
        "chars"
      ],
      "metadata": {
        "id": "TudtcfRXmlJJ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "78e8a9ec-d42e-4e34-cee9-853eab18453d"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.RaggedTensor [[b'a', b'b', b'c', b'd', b'e', b'f', b'g'], [b'x', b'y', b'z']]>"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "Now create the [`tf.keras.layers.StringLookup`](https://www.tensorflow.org/api_docs/python/tf/keras/layers/StringLookup) layer:\n",
        "\n",
        "Since the goal of this assignment is to generate text, it will also be important to invert this representation and recover human-readable strings from it. For this you can use tf.keras.layers.StringLookup(..., invert=True).\n",
        "\n",
        "Note: Here instead of passing the original vocabulary generated with sorted(set(text)) use the get_vocabulary() method of the tf.keras.layers.StringLookup layer so that the [UNK] tokens is set the same way."
      ],
      "metadata": {
        "id": "3ekRBgD3xX44"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_string_lookup_layers(vocab):\n",
        "    \"\"\"\n",
        "    Creates StringLookup layers for encoding characters to IDs and decoding IDs back to characters.\n",
        "\n",
        "    Args:\n",
        "        vocab (list): List of unique characters in the dataset.\n",
        "\n",
        "    Returns:\n",
        "        ids_from_chars (tf.keras.layers.StringLookup): Converts characters to IDs.\n",
        "        chars_from_ids (tf.keras.layers.StringLookup): Converts IDs back to characters.\n",
        "    \"\"\"\n",
        "    ids_from_chars = tf.keras.layers.StringLookup(vocabulary=list(vocab), mask_token=None)\n",
        "    chars_from_ids = tf.keras.layers.StringLookup(vocabulary=ids_from_chars.get_vocabulary(), invert=True, mask_token=None)\n",
        "    return ids_from_chars, chars_from_ids"
      ],
      "metadata": {
        "id": "S4nkBgY80jKz"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def text_from_ids(ids, chars_from_ids):\n",
        "    \"\"\"\n",
        "    Converts a sequence of character IDs into a human-readable string.\n",
        "\n",
        "    Args:\n",
        "        ids (tf.Tensor): Tensor of character IDs.\n",
        "        chars_from_ids (tf.keras.layers.StringLookup): StringLookup layer to decode IDs.\n",
        "\n",
        "    Returns:\n",
        "        tf.Tensor: Decoded string.\n",
        "    \"\"\"\n",
        "    return tf.strings.reduce_join(chars_from_ids(ids), axis=-1)"
      ],
      "metadata": {
        "id": "yy3htlAl2LpY"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Create training examples and targets"
      ],
      "metadata": {
        "id": "WfPugm5F2tit"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Next divide the text into example sequences. Each input sequence will contain seq_length characters from the text.\n",
        "\n",
        "For each input sequence, the corresponding targets contain the same length of text, except shifted one character to the right.\n",
        "\n",
        "So break the text into chunks of seq_length+1. For example, say seq_length is 4 and our text is \"Hello\". The input sequence would be \"Hell\", and the target sequence \"ello\".\n",
        "\n",
        "To do this first use the tf.data.Dataset.from_tensor_slices function to convert the text vector into a stream of character indices."
      ],
      "metadata": {
        "id": "LgO2xqIl26Q2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "For training you'll need a dataset of (input, label) pairs. Where input and label are sequences. At each time step the input is the current character and the label is the next character.\n",
        "\n",
        "Here's a function that takes a sequence as input, duplicates, and shifts it to align the input and label for each timestep:"
      ],
      "metadata": {
        "id": "-zlTqVW-4JYc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def split_input_target(sequence):\n",
        "    input_text = sequence[:-1]\n",
        "    target_text = sequence[1:]\n",
        "    return input_text, target_text"
      ],
      "metadata": {
        "id": "M5nUqJy94jz4"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Convert Text to Numerical Sequences"
      ],
      "metadata": {
        "id": "cYBKXEO2-jo1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize mapping layers\n",
        "vocab = sorted(set(text))\n",
        "ids_from_chars, chars_from_ids = get_string_lookup_layers(vocab)\n",
        "\n",
        "\n",
        "\n",
        "# Convert text to character IDs\n",
        "all_ids = ids_from_chars(tf.strings.unicode_split(text, 'UTF-8'))\n",
        "\n",
        "# Create a dataset from the character IDs\n",
        "ids_dataset = tf.data.Dataset.from_tensor_slices(all_ids)"
      ],
      "metadata": {
        "id": "m07YEPAn95W9"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Create Sequences for Training"
      ],
      "metadata": {
        "id": "m4SjKO_U-3_E"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "seq_length = 100  # Length of each training sequence\n",
        "\n",
        "# Batch sequences (each sequence is seq_length + 1)\n",
        "sequences = ids_dataset.batch(seq_length + 1, drop_remainder=True)\n",
        "\n",
        "# Map dataset to input-target format\n",
        "dataset = sequences.map(split_input_target)\n",
        "\n",
        "# Print sample input-output pairs\n",
        "for input_example, target_example in dataset.take(1):\n",
        "    print(\"Input :\", tf.strings.reduce_join(chars_from_ids(input_example)).numpy().decode('utf-8'))\n",
        "    print(\"Target:\", tf.strings.reduce_join(chars_from_ids(target_example)).numpy().decode('utf-8'))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QfiiwVFB-5lc",
        "outputId": "9a939e03-f1cb-498c-ab64-ba1cc6d3b512"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input : First Citizen:\n",
            "Before we proceed any further, hear me speak.\n",
            "\n",
            "All:\n",
            "Speak, speak.\n",
            "\n",
            "First Citizen:\n",
            "You\n",
            "Target: irst Citizen:\n",
            "Before we proceed any further, hear me speak.\n",
            "\n",
            "All:\n",
            "Speak, speak.\n",
            "\n",
            "First Citizen:\n",
            "You \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Create RNN Models"
      ],
      "metadata": {
        "id": "dI6ui5GUmmTy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Before training our text generation models, we need to set up key parameters and prepare the dataset. These parameters control the size of batches, buffer size for shuffling, embedding dimensions, and the number of units in the recurrent layer."
      ],
      "metadata": {
        "id": "xURojLcPEE7_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Customizing Parameters for Optimization\n",
        "\n",
        "- Increase RNN_UNITS if the model is underfitting (not capturing enough detail).\n",
        "- Decrease BATCH_SIZE if memory usage is too high (e.g., when using large RNN units).\n",
        "- Adjust EMBEDDING_DIM to experiment with the quality of learned character representations.\n",
        "\n",
        "Tip: Start with these values, then fine-tune based on model performance and available computational resources!"
      ],
      "metadata": {
        "id": "qGISBf1UEYFX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "BATCH_SIZE = 64\n",
        "BUFFER_SIZE = 10000\n",
        "VOCAB_SIZE = len(vocab) + 1\n",
        "DATASET_SIZE = sum(1 for _ in dataset)\n",
        "\n",
        "shuffled_dataset = dataset.shuffle(BUFFER_SIZE)\n",
        "\n",
        "train_dataset = shuffled_dataset.take(int(0.9 * DATASET_SIZE) )\n",
        "val_dataset = shuffled_dataset.skip(int(0.1 * DATASET_SIZE) )\n",
        "\n",
        "# Prepare dataset for training\n",
        "training_dataset = (train_dataset\n",
        "                 .batch(BATCH_SIZE, drop_remainder=True)\n",
        "                 .prefetch(tf.data.experimental.AUTOTUNE))\n",
        "# Prepare dataset for validation\n",
        "validation_dataset = (val_dataset\n",
        "               .batch(BATCH_SIZE, drop_remainder=True)\n",
        "               .prefetch(tf.data.experimental.AUTOTUNE))"
      ],
      "metadata": {
        "id": "M4WOh946DkY6"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# LSTM-Based Model"
      ],
      "metadata": {
        "id": "dVgheHjADca6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Long Short-Term Memory (LSTM) is a type of Recurrent Neural Network (RNN) designed to capture long-range dependencies in sequential data. Unlike traditional RNNs, which struggle with vanishing gradients, LSTMs use gates (input, forget, and output gates) to regulate the flow of information, making them highly effective for text generation tasks.\n",
        "\n",
        "This function defines an LSTM-based neural network for text generation.\n",
        "\n",
        "**Function Overview:**\n",
        "* [`tf.keras.layers.Embedding`](https://www.tensorflow.org/api_docs/python/tf/keras/layers/Embedding): This is the input layer, consisting of a trainable lookup table that maps the numbers of each character to a vector with `embedding_dim` dimensions.\n",
        "* [`tf.keras.layers.LSTM`](https://www.tensorflow.org/api_docs/python/tf/keras/layers/LSTM): Our LSTM network, with size `units=rnn_units`.\n",
        "* [`tf.keras.layers.Dense`](https://www.tensorflow.org/api_docs/python/tf/keras/layers/Dense): The output layer, with `vocab_size` outputs.\n",
        "\n",
        "\n",
        "<img src=\"https://raw.githubusercontent.com/aamini/introtodeeplearning/2019/lab1/img/lstm_unrolled-01-01.png\" alt=\"Drawing\"/>"
      ],
      "metadata": {
        "id": "KkpCV_AOFUv8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def build_lstm_model(vocab_size, embedding_dim, rnn_units, batch_size):\n",
        "    \"\"\"\n",
        "    Builds an LSTM-based text generation model with stacked LSTM layers.\n",
        "\n",
        "    Parameters:\n",
        "    - vocab_size (int): Number of unique characters in the vocabulary.\n",
        "    - embedding_dim (int): Dimension of the word/character embeddings.\n",
        "    - rnn_units (int): Number of units in the LSTM layer.\n",
        "    - batch_size (int): Number of sequences processed in parallel.\n",
        "\n",
        "    Returns:\n",
        "    - tf.keras.Model: Compiled LSTM model.\n",
        "    \"\"\"\n",
        "    model = tf.keras.Sequential([\n",
        "        tf.keras.layers.Embedding(vocab_size, embedding_dim),\n",
        "        tf.keras.layers.LSTM(rnn_units,\n",
        "                             return_sequences=True,\n",
        "                             stateful=True,\n",
        "                             recurrent_initializer='glorot_uniform'),\n",
        "        tf.keras.layers.LSTM(rnn_units,\n",
        "                             return_sequences=True,\n",
        "                             stateful=True,\n",
        "                             recurrent_initializer='glorot_uniform'),\n",
        "        tf.keras.layers.LSTM(rnn_units,\n",
        "                             return_sequences=True,\n",
        "                             stateful=True,\n",
        "                             recurrent_initializer='glorot_uniform'),\n",
        "        tf.keras.layers.Dense(vocab_size)\n",
        "    ])\n",
        "    return model"
      ],
      "metadata": {
        "id": "61PUIO1vFW3c"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Customization & Optimization**\n",
        "\n",
        "These experiments are required to improve model performance. You must conduct the following experiments and document your findings:\n",
        "\n",
        "- Increase RNN_UNITS : This enhances the model’s ability to recognize deeper patterns but may increase training time.\n",
        "- Experiment with EMBEDDING_DIM : Adjusting this can improve the quality of character representation.\n",
        "- Stack multiple LSTM layers : This can help the model understand text more effectively.\n",
        "\n",
        "**Submission Requirements**\n",
        "\n",
        "- Perform at least three trials varying the parameters above.\n",
        "- Keep the most performing plots and outputs in your Jupyter Notebook.\n"
      ],
      "metadata": {
        "id": "lPG4uS2rGVZH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# GRU-Based Model"
      ],
      "metadata": {
        "id": "7zitjaXenJJv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "What is GRU?\n",
        "\n",
        "Gated Recurrent Units (GRU) are a simplified version of LSTMs that combine the forget and input gates into a single update gate. This makes GRUs:\n",
        "\n",
        "- Faster and more efficient than LSTMs\n",
        "- Perform well on shorter sequences\n",
        "- Require fewer computational resources\n",
        "\n",
        "**Model Comparison (LSTM vs. GRU)**\n",
        "\n",
        "To evaluate the differences between LSTM and GRU models, compare them using:\n",
        "\n",
        "- Model Size & Number of Parameters : Use model.summary() to check the number of trainable parameters in each model.\n",
        "- Training Speed & Efficiency : Measure training time per epoch for both models."
      ],
      "metadata": {
        "id": "yizzA4azG6rm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def build_gru_model(vocab_size, embedding_dim, rnn_units, batch_size):\n",
        "    \"\"\"\n",
        "    Builds a GRU-based text generation model.\n",
        "\n",
        "    Parameters:\n",
        "    - vocab_size (int): Number of unique characters in the vocabulary.\n",
        "    - embedding_dim (int): Dimension of the word/character embeddings.\n",
        "    - rnn_units (int): Number of units in the GRU layer.\n",
        "    - batch_size (int): Number of sequences processed in parallel.\n",
        "\n",
        "    Returns:\n",
        "    - tf.keras.Model: Compiled GRU model.\n",
        "    \"\"\"\n",
        "    model = tf.keras.Sequential([\n",
        "        tf.keras.layers.Embedding(vocab_size, embedding_dim),\n",
        "        tf.keras.layers.GRU(rnn_units,\n",
        "                            return_sequences=True,\n",
        "                            stateful=True,\n",
        "                            recurrent_initializer='glorot_uniform'),\n",
        "        tf.keras.layers.Dense(vocab_size)\n",
        "    ])\n",
        "    return model"
      ],
      "metadata": {
        "id": "xlq36A5bFpRa"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Bidirectional Model\n",
        "\n",
        "\n",
        "<img src=\"https://www.researchgate.net/publication/342646275/figure/fig4/AS:962238546464790@1606426955772/Comparison-between-LSTM-and-Bi-LSTM-networks-recreated-after-33.png\" alt=\"Drawing\"/>\n"
      ],
      "metadata": {
        "id": "V5WHnOJzmxNy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**What is Bidirectional RNN?**\n",
        "\n",
        "A Bidirectional Recurrent Neural Network (BiRNN) is an extension of a standard RNN that processes input sequences in both forward and backward directions. This means that at each time step, the model considers both past (left-to-right) and future (right-to-left) context, leading to better performance in many sequence-related tasks, including text generation.\n",
        "\n",
        "**Why Use Bidirectional RNNs?**\n",
        "\n",
        "Bidirectional RNNs are used because they capture dependencies in the input data from both directions, which is crucial for understanding context. This makes them particularly useful in tasks like language processing, where the meaning of a word can depend on both the words that come before and after it.\n",
        "\n",
        "For example, in the sentence \"The cat sat on the mat,\" the word \"mat\" is influenced by both the preceding words (\"The cat sat on the\") and what could potentially follow (e.g., \"and looked at the mouse\").\n",
        "\n",
        "Example\n",
        "\n",
        "Let's consider a sentence: \"He opened the door.\"\n",
        "\n",
        "Simple RNN: It processes the sentence from left to right, word by word:\n",
        "\n",
        "He → opened → the → door\n",
        "Each word is processed based on the previous word.\n",
        "\n",
        "Bidirectional RNN: It processes the sentence in both directions:\n",
        "\n",
        "Forward pass: He → opened → the → door\n",
        "\n",
        "Backward pass: door → the → opened → He\n",
        "\n",
        "The output at each time step is a combination of the information from both the forward and backward passes, allowing the network to understand the context better. For instance, knowing that \"door\" follows \"the\" helps confirm that \"opened\" likely refers to a physical action, not a metaphorical one."
      ],
      "metadata": {
        "id": "3JFUjj1II4Jk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Choosing the Base RNN for Bidirectional Processing:**\n",
        "\n",
        "One key flexibility of Bidirectional RNNs is that you can choose any recurrent architecture (LSTM, GRU, Simple RNN) as the base model.\n",
        "\n",
        "- Bidirectional LSTM: Best for handling long-range dependencies.\n",
        "- Bidirectional GRU: More computationally efficient than LSTM.\n",
        "- Bidirectional Simple RNN: Less commonly used due to vanishing gradient issues.\n",
        "\n",
        "Note: To determine the best base model for the Bidirectional RNN, you must evaluate the results from your previous experiments with different architectures (eg. LSTM and GRU).\n",
        "\n",
        "Review previous results from training LSTM and GRU models.\n",
        "Compare their performance in terms of:\n",
        "\n",
        "- Training time per epoch\n",
        "- Model size (number of parameters)\n",
        "- Loss and accuracy on validation data\n",
        "- Quality of generated text\n",
        "\n",
        "Based on your findings, select the best-performing model as the base architecture for the Bidirectional RNN.\n"
      ],
      "metadata": {
        "id": "Cie7twv5J__0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def build_birnn_model(vocab_size, embedding_dim, rnn_units, batch_size, rnn_type=\"GRU\"):\n",
        "    \"\"\"\n",
        "    Builds a Bidirectional RNN-based text generation model.\n",
        "\n",
        "    Parameters:\n",
        "    - vocab_size (int): Number of unique characters in the vocabulary.\n",
        "    - embedding_dim (int): Dimension of the word/character embeddings.\n",
        "    - rnn_units (int): Number of units in the RNN layer.\n",
        "    - batch_size (int): Number of sequences processed in parallel.\n",
        "    - rnn_type (str): Type of RNN to use as the base (options: \"LSTM\", \"GRU\").\n",
        "\n",
        "    Returns:\n",
        "    - tf.keras.Model: Compiled Bidirectional RNN model.\n",
        "    \"\"\"\n",
        "    model = tf.keras.Sequential([\n",
        "        tf.keras.layers.Embedding(vocab_size, embedding_dim),\n",
        "    ])\n",
        "\n",
        "    if rnn_type == \"LSTM\":\n",
        "        rnn_layer = tf.keras.layers.LSTM(rnn_units,\n",
        "                                        return_sequences=True,\n",
        "                                        stateful=True,\n",
        "                                        recurrent_initializer='glorot_uniform')\n",
        "    elif rnn_type == \"GRU\":\n",
        "        rnn_layer = tf.keras.layers.GRU(rnn_units,\n",
        "                                       return_sequences=True,\n",
        "                                       stateful=True,\n",
        "                                       recurrent_initializer='glorot_uniform')\n",
        "    else:\n",
        "        raise ValueError(\"rnn_type must be 'LSTM' or 'GRU'\")\n",
        "\n",
        "    model.add(tf.keras.layers.Bidirectional(rnn_layer))\n",
        "    model.add(tf.keras.layers.Dense(vocab_size))\n",
        "\n",
        "    return model"
      ],
      "metadata": {
        "id": "m7CRdaDrHgvz"
      },
      "execution_count": 62,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Evaluating the Models"
      ],
      "metadata": {
        "id": "tDsQJY7wnZxu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Model Selection"
      ],
      "metadata": {
        "id": "N_dYS7PZHPzL"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "77649f45"
      },
      "source": [
        "EMBEDDING_DIM = 256\n",
        "RNN_UNITS = 1024"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Choose model type\n",
        "model_type = \"LSTM\"\n",
        "\n",
        "if model_type == \"LSTM\":\n",
        "    model = build_lstm_model(VOCAB_SIZE, EMBEDDING_DIM, RNN_UNITS, BATCH_SIZE)\n",
        "elif model_type == \"GRU\":\n",
        "    model = build_gru_model(VOCAB_SIZE, EMBEDDING_DIM, RNN_UNITS, BATCH_SIZE)\n",
        "elif model_type == \"BiLSTM\":\n",
        "    model = build_birnn_model(VOCAB_SIZE, EMBEDDING_DIM, RNN_UNITS, BATCH_SIZE)\n",
        "\n",
        "print(f\"Using {model_type} model for training.\")"
      ],
      "metadata": {
        "id": "Mmcr-HmQnepP",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9b005ce9-107a-42a1-ba48-c49423e364a5"
      },
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using LSTM model for training.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Choose model type\n",
        "model_type = \"GRU\"\n",
        "\n",
        "if model_type == \"LSTM\":\n",
        "    model = build_lstm_model(VOCAB_SIZE, EMBEDDING_DIM, RNN_UNITS, BATCH_SIZE)\n",
        "elif model_type == \"GRU\":\n",
        "    model = build_gru_model(VOCAB_SIZE, EMBEDDING_DIM, RNN_UNITS, BATCH_SIZE)\n",
        "elif model_type == \"BiLSTM\":\n",
        "    model = build_birnn_model(VOCAB_SIZE, EMBEDDING_DIM, RNN_UNITS, BATCH_SIZE)\n",
        "\n",
        "print(f\"Using {model_type} model for training.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GK8OOFxfJaDi",
        "outputId": "985c412d-737c-4647-b626-8431a32d2bb2"
      },
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using GRU model for training.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Choose model type\n",
        "model_type = \"BiGRU\"\n",
        "\n",
        "if model_type == \"LSTM\":\n",
        "    model = build_lstm_model(VOCAB_SIZE, EMBEDDING_DIM, RNN_UNITS, BATCH_SIZE)\n",
        "elif model_type == \"GRU\":\n",
        "    model = build_gru_model(VOCAB_SIZE, EMBEDDING_DIM, RNN_UNITS, BATCH_SIZE)\n",
        "elif model_type == \"BiGRU\":\n",
        "    model = build_birnn_model(VOCAB_SIZE, EMBEDDING_DIM, RNN_UNITS, BATCH_SIZE)\n",
        "\n",
        "print(f\"Using {model_type} model for training.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OekA-PeQLlYH",
        "outputId": "2a3da1b0-67fd-40db-f0db-cdb82d18da10"
      },
      "execution_count": 64,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using BiGRU model for training.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Compile & Train the Model"
      ],
      "metadata": {
        "id": "h5UaTlKZLQxs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def compile_and_train(model, path, EPOCHS = 30):\n",
        "  # Define loss function\n",
        "  def loss(labels, logits):\n",
        "    return tf.keras.losses.sparse_categorical_crossentropy(labels, logits, from_logits=True)\n",
        "\n",
        "  # Compile the model\n",
        "  model.compile(optimizer='adam', loss=loss)\n",
        "\n",
        "  # Ensure checkpoint directory exists\n",
        "  checkpoint_dir = f'./{path}'\n",
        "  os.makedirs(checkpoint_dir, exist_ok=True)\n",
        "\n",
        "  checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt_{epoch}\")\n",
        "\n",
        "  checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n",
        "    filepath=checkpoint_prefix + '.weights.h5',\n",
        "    save_weights_only=True\n",
        "  )\n",
        "\n",
        "  # Train the model\n",
        "  history = model.fit(\n",
        "    training_dataset,\n",
        "    validation_data=validation_dataset,\n",
        "    epochs=EPOCHS,\n",
        "    callbacks=[checkpoint_callback]\n",
        ")\n",
        "\n",
        "  return history"
      ],
      "metadata": {
        "id": "0mEIvU_3LPyL"
      },
      "execution_count": 55,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"Compiling and training the {model_type} model...\")\n",
        "history = compile_and_train(model, path=f'{model_type.lower()}_checkpoint', EPOCHS=30)\n",
        "\n",
        "print(f\"Training finished for the {model_type} model.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X06ZMWwmTvJZ",
        "outputId": "376653e1-304d-448e-f398-0e9477325943"
      },
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Compiling and training the LSTM model...\n",
            "Epoch 1/30\n",
            "\u001b[1m155/155\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m75s\u001b[0m 450ms/step - loss: 3.5830 - val_loss: 2.6883\n",
            "Epoch 2/30\n",
            "\u001b[1m155/155\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m74s\u001b[0m 469ms/step - loss: 2.4798 - val_loss: 2.1343\n",
            "Epoch 3/30\n",
            "\u001b[1m155/155\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m70s\u001b[0m 444ms/step - loss: 2.0632 - val_loss: 1.8667\n",
            "Epoch 4/30\n",
            "\u001b[1m155/155\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m70s\u001b[0m 443ms/step - loss: 1.8193 - val_loss: 1.6887\n",
            "Epoch 5/30\n",
            "\u001b[1m155/155\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m75s\u001b[0m 471ms/step - loss: 1.6623 - val_loss: 1.5670\n",
            "Epoch 6/30\n",
            "\u001b[1m155/155\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m76s\u001b[0m 479ms/step - loss: 1.5513 - val_loss: 1.4936\n",
            "Epoch 7/30\n",
            "\u001b[1m155/155\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m74s\u001b[0m 465ms/step - loss: 1.4819 - val_loss: 1.4362\n",
            "Epoch 8/30\n",
            "\u001b[1m155/155\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m78s\u001b[0m 489ms/step - loss: 1.4348 - val_loss: 1.3971\n",
            "Epoch 9/30\n",
            "\u001b[1m155/155\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m73s\u001b[0m 457ms/step - loss: 1.3966 - val_loss: 1.3651\n",
            "Epoch 10/30\n",
            "\u001b[1m155/155\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m77s\u001b[0m 487ms/step - loss: 1.3645 - val_loss: 1.3418\n",
            "Epoch 11/30\n",
            "\u001b[1m155/155\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m72s\u001b[0m 451ms/step - loss: 1.3399 - val_loss: 1.3175\n",
            "Epoch 12/30\n",
            "\u001b[1m155/155\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m71s\u001b[0m 447ms/step - loss: 1.3206 - val_loss: 1.2966\n",
            "Epoch 13/30\n",
            "\u001b[1m155/155\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m76s\u001b[0m 477ms/step - loss: 1.2980 - val_loss: 1.2751\n",
            "Epoch 14/30\n",
            "\u001b[1m155/155\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m76s\u001b[0m 479ms/step - loss: 1.2754 - val_loss: 1.2591\n",
            "Epoch 15/30\n",
            "\u001b[1m155/155\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m78s\u001b[0m 490ms/step - loss: 1.2635 - val_loss: 1.2407\n",
            "Epoch 16/30\n",
            "\u001b[1m155/155\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m74s\u001b[0m 471ms/step - loss: 1.2452 - val_loss: 1.2256\n",
            "Epoch 17/30\n",
            "\u001b[1m155/155\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m77s\u001b[0m 488ms/step - loss: 1.2283 - val_loss: 1.2092\n",
            "Epoch 18/30\n",
            "\u001b[1m155/155\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m77s\u001b[0m 482ms/step - loss: 1.2127 - val_loss: 1.1887\n",
            "Epoch 19/30\n",
            "\u001b[1m155/155\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m72s\u001b[0m 453ms/step - loss: 1.1989 - val_loss: 1.1744\n",
            "Epoch 20/30\n",
            "\u001b[1m155/155\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m71s\u001b[0m 446ms/step - loss: 1.1789 - val_loss: 1.1548\n",
            "Epoch 21/30\n",
            "\u001b[1m155/155\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m76s\u001b[0m 479ms/step - loss: 1.1614 - val_loss: 1.1355\n",
            "Epoch 22/30\n",
            "\u001b[1m155/155\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m76s\u001b[0m 479ms/step - loss: 1.1439 - val_loss: 1.1200\n",
            "Epoch 23/30\n",
            "\u001b[1m155/155\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m76s\u001b[0m 479ms/step - loss: 1.1291 - val_loss: 1.1043\n",
            "Epoch 24/30\n",
            "\u001b[1m155/155\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m73s\u001b[0m 461ms/step - loss: 1.1076 - val_loss: 1.0859\n",
            "Epoch 25/30\n",
            "\u001b[1m155/155\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m76s\u001b[0m 483ms/step - loss: 1.0918 - val_loss: 1.0652\n",
            "Epoch 26/30\n",
            "\u001b[1m155/155\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m76s\u001b[0m 480ms/step - loss: 1.0745 - val_loss: 1.0409\n",
            "Epoch 27/30\n",
            "\u001b[1m155/155\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m76s\u001b[0m 482ms/step - loss: 1.0499 - val_loss: 1.0240\n",
            "Epoch 28/30\n",
            "\u001b[1m155/155\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m78s\u001b[0m 488ms/step - loss: 1.0349 - val_loss: 1.0011\n",
            "Epoch 29/30\n",
            "\u001b[1m155/155\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m72s\u001b[0m 456ms/step - loss: 1.0106 - val_loss: 0.9799\n",
            "Epoch 30/30\n",
            "\u001b[1m155/155\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m75s\u001b[0m 476ms/step - loss: 0.9868 - val_loss: 0.9591\n",
            "Training finished for the LSTM model.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"Compiling and training the {model_type} model...\")\n",
        "history = compile_and_train(model, path=f'{model_type.lower()}_checkpoint', EPOCHS=30)\n",
        "\n",
        "print(f\"Training finished for the {model_type} model.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-q0ErOAjeC66",
        "outputId": "38ab77b4-127d-48d5-f20b-a8131d1ee258"
      },
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Compiling and training the GRU model...\n",
            "Epoch 1/30\n",
            "\u001b[1m155/155\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 101ms/step - loss: 3.1680 - val_loss: 2.0566\n",
            "Epoch 2/30\n",
            "\u001b[1m155/155\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 96ms/step - loss: 1.9648 - val_loss: 1.7360\n",
            "Epoch 3/30\n",
            "\u001b[1m155/155\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 98ms/step - loss: 1.6878 - val_loss: 1.5615\n",
            "Epoch 4/30\n",
            "\u001b[1m155/155\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 94ms/step - loss: 1.5431 - val_loss: 1.4630\n",
            "Epoch 5/30\n",
            "\u001b[1m155/155\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 97ms/step - loss: 1.4577 - val_loss: 1.3987\n",
            "Epoch 6/30\n",
            "\u001b[1m155/155\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 95ms/step - loss: 1.3980 - val_loss: 1.3497\n",
            "Epoch 7/30\n",
            "\u001b[1m155/155\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 95ms/step - loss: 1.3482 - val_loss: 1.3073\n",
            "Epoch 8/30\n",
            "\u001b[1m155/155\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 95ms/step - loss: 1.3104 - val_loss: 1.2697\n",
            "Epoch 9/30\n",
            "\u001b[1m155/155\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 96ms/step - loss: 1.2793 - val_loss: 1.2436\n",
            "Epoch 10/30\n",
            "\u001b[1m155/155\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 99ms/step - loss: 1.2500 - val_loss: 1.2112\n",
            "Epoch 11/30\n",
            "\u001b[1m155/155\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 95ms/step - loss: 1.2204 - val_loss: 1.1814\n",
            "Epoch 12/30\n",
            "\u001b[1m155/155\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 94ms/step - loss: 1.1897 - val_loss: 1.1520\n",
            "Epoch 13/30\n",
            "\u001b[1m155/155\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 96ms/step - loss: 1.1638 - val_loss: 1.1276\n",
            "Epoch 14/30\n",
            "\u001b[1m155/155\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 94ms/step - loss: 1.1361 - val_loss: 1.1001\n",
            "Epoch 15/30\n",
            "\u001b[1m155/155\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 95ms/step - loss: 1.1064 - val_loss: 1.0663\n",
            "Epoch 16/30\n",
            "\u001b[1m155/155\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 95ms/step - loss: 1.0770 - val_loss: 1.0405\n",
            "Epoch 17/30\n",
            "\u001b[1m155/155\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 96ms/step - loss: 1.0522 - val_loss: 1.0061\n",
            "Epoch 18/30\n",
            "\u001b[1m155/155\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 98ms/step - loss: 1.0195 - val_loss: 0.9784\n",
            "Epoch 19/30\n",
            "\u001b[1m155/155\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 95ms/step - loss: 0.9931 - val_loss: 0.9483\n",
            "Epoch 20/30\n",
            "\u001b[1m155/155\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 96ms/step - loss: 0.9642 - val_loss: 0.9183\n",
            "Epoch 21/30\n",
            "\u001b[1m155/155\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 94ms/step - loss: 0.9348 - val_loss: 0.8879\n",
            "Epoch 22/30\n",
            "\u001b[1m155/155\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 94ms/step - loss: 0.9039 - val_loss: 0.8606\n",
            "Epoch 23/30\n",
            "\u001b[1m155/155\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 97ms/step - loss: 0.8770 - val_loss: 0.8378\n",
            "Epoch 24/30\n",
            "\u001b[1m155/155\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 95ms/step - loss: 0.8558 - val_loss: 0.8096\n",
            "Epoch 25/30\n",
            "\u001b[1m155/155\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 95ms/step - loss: 0.8285 - val_loss: 0.7909\n",
            "Epoch 26/30\n",
            "\u001b[1m155/155\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 97ms/step - loss: 0.8055 - val_loss: 0.7723\n",
            "Epoch 27/30\n",
            "\u001b[1m155/155\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 95ms/step - loss: 0.7858 - val_loss: 0.7515\n",
            "Epoch 28/30\n",
            "\u001b[1m155/155\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 95ms/step - loss: 0.7665 - val_loss: 0.7359\n",
            "Epoch 29/30\n",
            "\u001b[1m155/155\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 95ms/step - loss: 0.7530 - val_loss: 0.7192\n",
            "Epoch 30/30\n",
            "\u001b[1m155/155\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 94ms/step - loss: 0.7348 - val_loss: 0.7078\n",
            "Training finished for the GRU model.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"Compiling and training the {model_type} model...\")\n",
        "history = compile_and_train(model, path=f'{model_type.lower()}_checkpoint', EPOCHS=30)\n",
        "\n",
        "print(f\"Training finished for the {model_type} model.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LNBd9upmmTEB",
        "outputId": "7beb2165-4f7f-48b0-de9a-d32b071725fa"
      },
      "execution_count": 65,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Compiling and training the BiGRU model...\n",
            "Epoch 1/30\n",
            "\u001b[1m155/155\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m35s\u001b[0m 202ms/step - loss: 2.1531 - val_loss: 0.0854\n",
            "Epoch 2/30\n",
            "\u001b[1m155/155\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m32s\u001b[0m 181ms/step - loss: 0.0769 - val_loss: 0.0635\n",
            "Epoch 3/30\n",
            "\u001b[1m155/155\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m31s\u001b[0m 192ms/step - loss: 0.0614 - val_loss: 0.0570\n",
            "Epoch 4/30\n",
            "\u001b[1m155/155\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m32s\u001b[0m 190ms/step - loss: 0.0561 - val_loss: 0.0524\n",
            "Epoch 5/30\n",
            "\u001b[1m155/155\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m30s\u001b[0m 184ms/step - loss: 0.0517 - val_loss: 0.0490\n",
            "Epoch 6/30\n",
            "\u001b[1m155/155\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m31s\u001b[0m 187ms/step - loss: 0.0490 - val_loss: 0.0469\n",
            "Epoch 7/30\n",
            "\u001b[1m155/155\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m31s\u001b[0m 186ms/step - loss: 0.0469 - val_loss: 0.0440\n",
            "Epoch 8/30\n",
            "\u001b[1m155/155\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m31s\u001b[0m 187ms/step - loss: 0.0445 - val_loss: 0.0421\n",
            "Epoch 9/30\n",
            "\u001b[1m155/155\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m30s\u001b[0m 184ms/step - loss: 0.0432 - val_loss: 0.0403\n",
            "Epoch 10/30\n",
            "\u001b[1m155/155\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m31s\u001b[0m 186ms/step - loss: 0.0404 - val_loss: 0.0380\n",
            "Epoch 11/30\n",
            "\u001b[1m155/155\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m30s\u001b[0m 184ms/step - loss: 0.0394 - val_loss: 0.0366\n",
            "Epoch 12/30\n",
            "\u001b[1m155/155\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m30s\u001b[0m 186ms/step - loss: 0.0378 - val_loss: 0.0343\n",
            "Epoch 13/30\n",
            "\u001b[1m155/155\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m31s\u001b[0m 185ms/step - loss: 0.0350 - val_loss: 0.0328\n",
            "Epoch 14/30\n",
            "\u001b[1m155/155\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m30s\u001b[0m 185ms/step - loss: 0.0339 - val_loss: 0.0311\n",
            "Epoch 15/30\n",
            "\u001b[1m155/155\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m31s\u001b[0m 187ms/step - loss: 0.0319 - val_loss: 0.0296\n",
            "Epoch 16/30\n",
            "\u001b[1m155/155\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m30s\u001b[0m 185ms/step - loss: 0.0302 - val_loss: 0.0275\n",
            "Epoch 17/30\n",
            "\u001b[1m155/155\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m31s\u001b[0m 187ms/step - loss: 0.0283 - val_loss: 0.0262\n",
            "Epoch 18/30\n",
            "\u001b[1m155/155\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m31s\u001b[0m 186ms/step - loss: 0.0271 - val_loss: 0.0246\n",
            "Epoch 19/30\n",
            "\u001b[1m155/155\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m30s\u001b[0m 186ms/step - loss: 0.0253 - val_loss: 0.0226\n",
            "Epoch 20/30\n",
            "\u001b[1m155/155\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 188ms/step - loss: 0.0240 - val_loss: 0.0212\n",
            "Epoch 21/30\n",
            "\u001b[1m155/155\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m31s\u001b[0m 187ms/step - loss: 0.0219 - val_loss: 0.0205\n",
            "Epoch 22/30\n",
            "\u001b[1m155/155\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m30s\u001b[0m 181ms/step - loss: 0.0208 - val_loss: 0.0189\n",
            "Epoch 23/30\n",
            "\u001b[1m155/155\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m31s\u001b[0m 185ms/step - loss: 0.0193 - val_loss: 0.0179\n",
            "Epoch 24/30\n",
            "\u001b[1m155/155\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m31s\u001b[0m 186ms/step - loss: 0.0182 - val_loss: 0.0166\n",
            "Epoch 25/30\n",
            "\u001b[1m155/155\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m30s\u001b[0m 184ms/step - loss: 0.0169 - val_loss: 0.0156\n",
            "Epoch 26/30\n",
            "\u001b[1m155/155\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m31s\u001b[0m 188ms/step - loss: 0.0167 - val_loss: 0.0156\n",
            "Epoch 27/30\n",
            "\u001b[1m155/155\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m30s\u001b[0m 185ms/step - loss: 0.0156 - val_loss: 0.0147\n",
            "Epoch 28/30\n",
            "\u001b[1m155/155\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m31s\u001b[0m 189ms/step - loss: 0.0152 - val_loss: 0.0135\n",
            "Epoch 29/30\n",
            "\u001b[1m155/155\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m30s\u001b[0m 184ms/step - loss: 0.0146 - val_loss: 0.0136\n",
            "Epoch 30/30\n",
            "\u001b[1m155/155\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m30s\u001b[0m 186ms/step - loss: 0.0140 - val_loss: 0.0128\n",
            "Training finished for the BiGRU model.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Generate Text Using the Model"
      ],
      "metadata": {
        "id": "Ahq6S6erLqBr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class OneStepTextGenerator(tf.keras.Model):\n",
        "    def __init__(self, model, chars_from_ids, ids_from_chars, temperature=1.0):\n",
        "        \"\"\"\n",
        "        Initializes the OneStepTextGenerator.\n",
        "\n",
        "        Parameters:\n",
        "        - model: Trained text generation model (e.g., LSTM or GRU).\n",
        "        - chars_from_ids: Function mapping character IDs to characters.\n",
        "        - ids_from_chars: Function mapping characters to their respective IDs.\n",
        "        - temperature (float): Controls randomness in text generation.\n",
        "          -> Higher temperature (> 1.0): Produces more diverse and unpredictable text.\n",
        "          -> Lower temperature (< 1.0): Makes the model more confident but results in repetitive text.\n",
        "          -> Temperature = 1.0: Standard behavior without biasing randomness.\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "        self.model = model\n",
        "        self.chars_from_ids = chars_from_ids\n",
        "        self.ids_from_chars = ids_from_chars\n",
        "        self.temperature = temperature\n",
        "\n",
        "    def generate_text(self, start_string, num_generate=1000):\n",
        "        \"\"\"\n",
        "        Generates text one character at a time based on a starting string.\n",
        "\n",
        "        Parameters:\n",
        "        - start_string (str): Initial text prompt.\n",
        "        - num_generate (int): Number of characters to generate.\n",
        "\n",
        "        Returns:\n",
        "        - str: Generated text.\n",
        "        \"\"\"\n",
        "        # Convert start string to tensor representation\n",
        "        input_eval = tf.expand_dims(self.ids_from_chars(tf.strings.unicode_split(start_string, 'UTF-8')), 0)\n",
        "        text_generated = []\n",
        "\n",
        "        # Initial hidden state (None allows automatic initialization)\n",
        "        states = None\n",
        "\n",
        "        for _ in range(num_generate):\n",
        "            # Get model predictions and hidden state\n",
        "            # predictions, states = self.model(input_eval, states=states, return_state=True)\n",
        "            predictions = self.model(input_eval)\n",
        "\n",
        "            # Adjust predictions using temperature\n",
        "            predictions = predictions[:, -1, :] / self.temperature\n",
        "\n",
        "            # Sample the next character ID from a probability distribution\n",
        "            predicted_id = tf.random.categorical(predictions, num_samples=1)[-1, 0].numpy()\n",
        "\n",
        "            # Use the predicted ID as next input\n",
        "            input_eval = tf.expand_dims([predicted_id], 0)\n",
        "\n",
        "            # Convert ID back to character and append to output\n",
        "            text_generated.append(self.chars_from_ids(predicted_id).numpy().decode('utf-8'))\n",
        "\n",
        "        return start_string + ''.join(text_generated)\n"
      ],
      "metadata": {
        "id": "MssY2FXJLs1Q"
      },
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Evaluate Model Performance"
      ],
      "metadata": {
        "id": "fhvfs2NkL7kw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define loss function\n",
        "loss = tf.keras.losses.sparse_categorical_crossentropy"
      ],
      "metadata": {
        "id": "sBlcVn93Pu-J"
      },
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Build and summarize LSTM model\n",
        "lstm_model_compare = build_lstm_model(VOCAB_SIZE, EMBEDDING_DIM, RNN_UNITS, BATCH_SIZE)\n",
        "lstm_model_compare.build(tf.TensorShape([BATCH_SIZE, seq_length]))\n",
        "print(\"LSTM Model Summary\")\n",
        "lstm_model_compare.summary()\n",
        "\n",
        "# Measure training time for one epoch for LSTM\n",
        "print(\"LSTM Model:\")\n",
        "start_time_lstm = time.time()\n",
        "compile_and_train(lstm_model_compare, path='lstm_compare_checkpoint', EPOCHS=1)\n",
        "end_time_lstm = time.time()\n",
        "print(f\"Time per epoch (LSTM): {end_time_lstm - start_time_lstm:.2f} seconds\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 358
        },
        "id": "Wv5KcSl2M8rM",
        "outputId": "a184ea98-d95d-4370-af6a-73aaf172ff98"
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "LSTM Model Summary\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1mModel: \"sequential_8\"\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential_8\"</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
              "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
              "│ embedding_9 (\u001b[38;5;33mEmbedding\u001b[0m)         │ (\u001b[38;5;34m64\u001b[0m, \u001b[38;5;34m100\u001b[0m, \u001b[38;5;34m256\u001b[0m)         │        \u001b[38;5;34m16,896\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ lstm_16 (\u001b[38;5;33mLSTM\u001b[0m)                  │ (\u001b[38;5;34m64\u001b[0m, \u001b[38;5;34m100\u001b[0m, \u001b[38;5;34m1024\u001b[0m)        │     \u001b[38;5;34m5,246,976\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ lstm_17 (\u001b[38;5;33mLSTM\u001b[0m)                  │ (\u001b[38;5;34m64\u001b[0m, \u001b[38;5;34m100\u001b[0m, \u001b[38;5;34m1024\u001b[0m)        │     \u001b[38;5;34m8,392,704\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ lstm_18 (\u001b[38;5;33mLSTM\u001b[0m)                  │ (\u001b[38;5;34m64\u001b[0m, \u001b[38;5;34m100\u001b[0m, \u001b[38;5;34m1024\u001b[0m)        │     \u001b[38;5;34m8,392,704\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_8 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;34m64\u001b[0m, \u001b[38;5;34m100\u001b[0m, \u001b[38;5;34m66\u001b[0m)          │        \u001b[38;5;34m67,650\u001b[0m │\n",
              "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
              "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
              "│ embedding_9 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)         │ (<span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">100</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)         │        <span style=\"color: #00af00; text-decoration-color: #00af00\">16,896</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ lstm_16 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)                  │ (<span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">100</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1024</span>)        │     <span style=\"color: #00af00; text-decoration-color: #00af00\">5,246,976</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ lstm_17 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)                  │ (<span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">100</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1024</span>)        │     <span style=\"color: #00af00; text-decoration-color: #00af00\">8,392,704</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ lstm_18 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)                  │ (<span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">100</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1024</span>)        │     <span style=\"color: #00af00; text-decoration-color: #00af00\">8,392,704</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_8 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">100</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">66</span>)          │        <span style=\"color: #00af00; text-decoration-color: #00af00\">67,650</span> │\n",
              "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m22,116,930\u001b[0m (84.37 MB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">22,116,930</span> (84.37 MB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m22,116,930\u001b[0m (84.37 MB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">22,116,930</span> (84.37 MB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "LSTM Model:\n",
            "\u001b[1m155/155\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m75s\u001b[0m 456ms/step - loss: 3.3147 - val_loss: 2.2102\n",
            "Time per epoch (LSTM): 75.15 seconds\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Build and summarize GRU model\n",
        "gru_model_compare = build_gru_model(VOCAB_SIZE, EMBEDDING_DIM, RNN_UNITS, BATCH_SIZE)\n",
        "gru_model_compare.build(tf.TensorShape([BATCH_SIZE, seq_length]))\n",
        "print(\"\\n GRU Model Summary\")\n",
        "gru_model_compare.summary()\n",
        "\n",
        "# Measure training time for one epoch for GRU\n",
        "print(\"\\nGRU Model:\")\n",
        "start_time_gru = time.time()\n",
        "compile_and_train(gru_model_compare, path='gru_compare_checkpoint', EPOCHS=1)\n",
        "end_time_gru = time.time()\n",
        "print(f\"Time per epoch (GRU): {end_time_gru - start_time_gru:.2f} seconds\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 329
        },
        "id": "vaaZ39bmNRRJ",
        "outputId": "d455e694-e9ad-4c3d-a08b-90462563c786"
      },
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            " GRU Model Summary\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1mModel: \"sequential_10\"\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential_10\"</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
              "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
              "│ embedding_11 (\u001b[38;5;33mEmbedding\u001b[0m)        │ (\u001b[38;5;34m64\u001b[0m, \u001b[38;5;34m100\u001b[0m, \u001b[38;5;34m256\u001b[0m)         │        \u001b[38;5;34m16,896\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ gru_3 (\u001b[38;5;33mGRU\u001b[0m)                     │ (\u001b[38;5;34m64\u001b[0m, \u001b[38;5;34m100\u001b[0m, \u001b[38;5;34m1024\u001b[0m)        │     \u001b[38;5;34m3,938,304\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_10 (\u001b[38;5;33mDense\u001b[0m)                │ (\u001b[38;5;34m64\u001b[0m, \u001b[38;5;34m100\u001b[0m, \u001b[38;5;34m66\u001b[0m)          │        \u001b[38;5;34m67,650\u001b[0m │\n",
              "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
              "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
              "│ embedding_11 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)        │ (<span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">100</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)         │        <span style=\"color: #00af00; text-decoration-color: #00af00\">16,896</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ gru_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GRU</span>)                     │ (<span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">100</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1024</span>)        │     <span style=\"color: #00af00; text-decoration-color: #00af00\">3,938,304</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_10 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                │ (<span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">100</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">66</span>)          │        <span style=\"color: #00af00; text-decoration-color: #00af00\">67,650</span> │\n",
              "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m4,022,850\u001b[0m (15.35 MB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">4,022,850</span> (15.35 MB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m4,022,850\u001b[0m (15.35 MB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">4,022,850</span> (15.35 MB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "GRU Model:\n",
            "\u001b[1m155/155\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 98ms/step - loss: 3.1704 - val_loss: 2.0649\n",
            "Time per epoch (GRU): 18.20 seconds\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Build and summarize BiGRU model\n",
        "bigru_model_compare = build_birnn_model(VOCAB_SIZE, EMBEDDING_DIM, RNN_UNITS, BATCH_SIZE, rnn_type=\"GRU\")\n",
        "bigru_model_compare.build(tf.TensorShape([BATCH_SIZE, seq_length]))\n",
        "print(\"\\n BiGRU Model Summary\")\n",
        "bigru_model_compare.summary()\n",
        "\n",
        "# Measure training time for one epoch for BiGRU\n",
        "print(\"BiGRU Model:\")\n",
        "start_time_bigru = time.time()\n",
        "compile_and_train(bigru_model_compare, path='bigru_compare_checkpoint', EPOCHS=1)\n",
        "end_time_bigru = time.time()\n",
        "print(f\"Time per epoch (BiGRU): {end_time_bigru - start_time_bigru:.2f} seconds\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 312
        },
        "id": "egRDtZO1mvH1",
        "outputId": "fe8fa9fa-04bf-4e0e-f03f-4564d6dc5484"
      },
      "execution_count": 66,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            " BiGRU Model Summary\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1mModel: \"sequential_15\"\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential_15\"</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
              "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
              "│ embedding_16 (\u001b[38;5;33mEmbedding\u001b[0m)        │ (\u001b[38;5;34m64\u001b[0m, \u001b[38;5;34m100\u001b[0m, \u001b[38;5;34m256\u001b[0m)         │        \u001b[38;5;34m16,896\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ bidirectional_2 (\u001b[38;5;33mBidirectional\u001b[0m) │ (\u001b[38;5;34m64\u001b[0m, \u001b[38;5;34m100\u001b[0m, \u001b[38;5;34m2048\u001b[0m)        │     \u001b[38;5;34m7,876,608\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_15 (\u001b[38;5;33mDense\u001b[0m)                │ (\u001b[38;5;34m64\u001b[0m, \u001b[38;5;34m100\u001b[0m, \u001b[38;5;34m66\u001b[0m)          │       \u001b[38;5;34m135,234\u001b[0m │\n",
              "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
              "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
              "│ embedding_16 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)        │ (<span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">100</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)         │        <span style=\"color: #00af00; text-decoration-color: #00af00\">16,896</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ bidirectional_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Bidirectional</span>) │ (<span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">100</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2048</span>)        │     <span style=\"color: #00af00; text-decoration-color: #00af00\">7,876,608</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_15 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                │ (<span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">100</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">66</span>)          │       <span style=\"color: #00af00; text-decoration-color: #00af00\">135,234</span> │\n",
              "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m8,028,738\u001b[0m (30.63 MB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">8,028,738</span> (30.63 MB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m8,028,738\u001b[0m (30.63 MB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">8,028,738</span> (30.63 MB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "BiGRU Model:\n",
            "\u001b[1m155/155\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m32s\u001b[0m 187ms/step - loss: 2.1578 - val_loss: 0.0832\n",
            "Time per epoch (BiGRU): 32.24 seconds\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**1- Perplexity (PP)**\n",
        "\n",
        "TODO: Measures uncertainty in predicting next character for each model and leave the results in the Notebook.\n",
        "\n",
        "***Lower values = better model***"
      ],
      "metadata": {
        "id": "ucKVOi9KMHuv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def perplexity(logits, labels):\n",
        "    loss = tf.keras.losses.sparse_categorical_crossentropy(labels, logits, from_logits=True)\n",
        "    return np.exp(np.mean(loss))\n",
        "\n",
        "def evaluate_model_perplexity(model, dataset):\n",
        "    total_loss = 0.0\n",
        "    total_tokens = 0\n",
        "\n",
        "    for input_batch, label_batch in dataset:\n",
        "        logits = model(input_batch, training=False)\n",
        "        loss = perplexity(logits, label_batch)\n",
        "        total_loss += np.log(loss)\n",
        "        total_tokens += tf.size(label_batch).numpy()\n",
        "\n",
        "    avg_loss = total_loss / total_tokens\n",
        "    return np.exp(avg_loss)"
      ],
      "metadata": {
        "id": "7i38HrLPMAM7"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**2- Text Coherence & Fluency**\n",
        "\n",
        "Subjective evaluation by examining generated text\n",
        "\n",
        "**TODO: Generate at least 3 Samples (one from each model) and leave them in the Notebook.**"
      ],
      "metadata": {
        "id": "xCd0Am-KMaCb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Generate text from LSTM model\n",
        "lstm_generator = OneStepTextGenerator(model, chars_from_ids, ids_from_chars)\n",
        "lstm_generated_text = lstm_generator.generate_text(start_string=\"ROMEO:\")\n",
        "print(\"Generated text from LSTM model\")\n",
        "print(lstm_generated_text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Au76x9FQLuDj",
        "outputId": "5d1bae4e-4709-4224-fb5f-dfed7b1d321f"
      },
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generated text from LSTM model\n",
            "ROMEO:\n",
            "I see, my sovereign loyouthat my parts\n",
            "Is as boses by his vast all and desire\n",
            "To steal correction: there is our vain that I may fly.\n",
            "\n",
            "KING RICHARD III:\n",
            "Madam, your enemies!\n",
            "\n",
            "PAULINA:\n",
            "O sight! O thou think'st, gentle Flavery,\n",
            "Without a Paul, tell my sainted spirit,\n",
            "Dright with a meat; and here becomes,\n",
            "Most soft-as willingly as that may joy in their ates,\n",
            "The king she look'd of cord have broken death\n",
            "I speak as you both with no right and back;\n",
            "And ten things else of love is modested: meantime, I'll glly me to your mother. This foe\n",
            "Conot come.\n",
            "\n",
            "LUCIO:\n",
            "I'll rid he, lady.\n",
            "\n",
            "AUFIDIUS:\n",
            "He saw't; my requeens!\n",
            "\n",
            "TYRREL:\n",
            "Ay, good Patration, often stends that way should be well:\n",
            "Come, I'll respect me to your children.\n",
            "\n",
            "KATHARINA:\n",
            "I will unto the tribunes\n",
            "Do me with theretokes, my fair royal friends,\n",
            "The hand is better to me:\n",
            "Thou duty, man; meantimes, part, and yield consent\n",
            "While they do have my heart with thine eyes,\n",
            "When such a little father gave my place,\n",
            "And well come home, be gone; the king\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Generate text from GRU model\n",
        "gru_generator = OneStepTextGenerator(model, chars_from_ids, ids_from_chars)\n",
        "gru_generated_text = gru_generator.generate_text(start_string=\"ROMEO:\")\n",
        "print(\"\\n Generated text from GRU model\")\n",
        "print(gru_generated_text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B91gCp80MBRp",
        "outputId": "a19c42de-a323-41c0-c5ba-f663226f3ca2"
      },
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            " Generated text from GRU model\n",
            "ROMEO:\n",
            "So stood your brother is so early make ones.\n",
            "\n",
            "KING EDWARD IV:\n",
            "What is a wnightfor forth, a loveraughty head,\n",
            "That shall supress foot.\n",
            "\n",
            "MARCIUS:\n",
            "He's a better gone, my lord.\n",
            "\n",
            "KING RICHARD II:\n",
            "Say what our course of Lord!\n",
            "\n",
            "KING HENRY VI:\n",
            "O woe! O for that, I pray thee? speak.\n",
            "\n",
            "VINCENTIO:\n",
            "Why, so, no more of him.\n",
            "\n",
            "MENENIUS:\n",
            "Well, by your guests!\n",
            "\n",
            "First Murderer:\n",
            "How fares it to the face: therefore hent,\n",
            "As I myself aman.\n",
            "\n",
            "PROSPERO:\n",
            "Now there been sign.\n",
            "\n",
            "PETRUCHIO:\n",
            "Call them forth. A pinch'd brother Clarence, we are upon your oracle,\n",
            "Comen my eyes drop on the Raplish your army;\n",
            "O my most guard we banish him to your receiving to reshow Cominius since?\n",
            "\n",
            "Messenger:\n",
            "Sirrah, I pray you, and will hear some vow'd, I would not fly:\n",
            "Or the blessed sun urged the salt of our day's words:\n",
            "O, she lived us with flatterer loosers o'ergoade,\n",
            "Yea, and much;\n",
            "waight hat to make a better love the king.\n",
            "\n",
            "GLOUCESTER:\n",
            "Why, am I am age, seldom comes On you\n",
            "To do that kingly give you seen to take an hour\n",
            "says he \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Generate text from Bidirectional model\n",
        "birnn_generator = OneStepTextGenerator(model, chars_from_ids, ids_from_chars)\n",
        "birnn_generated_text = birnn_generator.generate_text(start_string=\"ROMEO:\")\n",
        "print(\"\\n Generated text from Bidirectional model\")\n",
        "print(birnn_generated_text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9AkDduv5L74J",
        "outputId": "fb1ffeb7-8682-49cc-bdc0-ca44bfa54580"
      },
      "execution_count": 68,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            " Generated text from Bidirectional model\n",
            "ROMEO:\n",
            "R\n",
            "R\n",
            "RORORORORORORORORORORO:\n",
            "Prisisisisisss's's's's's's's's's's's's's's's's's's's's's's's's's's's's's's's's's's's's's's's's's's's's's's's's's's's's's's's's's's's's's's's's's's's's's's's's's's's's's's's's's's's's's's's's's's's's's's's's's's's's's's's's's's's's s ciciaiainininininininsns.\n",
            "O\n",
            "O\n",
            "O\n",
            "OUSUSUSUSUSUSUSUSUSUSUSUSUSUSUSUSUSUSUSUSUSUSUSUSUSUSUSUSUSUSUSUSUSUSUSUSUSUSUSUSUSUSUSUSUSUSUSUSUSUSUSUSUSUSUSUSUSUSUSUSUSUSUSUSUSUSUSUSUSUSUSUSUSUSUSUSUSUSUSUSUSUSUSUSUSUSUSUSUSUSUSUSUSUSUSUSUSUSUSUSUSUSUSUSUSUSUSUSUSUSUSUSUSUSUSUSUSUSUSUSUSUSUSUSUSUSUSUSUSUSUSUSUSUSUSUSUSUSUSUSUSUSUSUSUSUSUSUSUSUSUSUSUSUSUSUSUSUSUSUSUSUSUSUSUSUSUSUSUSUSUSUSUSUSUSUSUSUSUSUSUSUSUSUSUSUSUSUSUSUSUSUSUSUSUSUSUSUSUSUSUSUSUSUSUSUSUSUSUS:\n",
            "O\n",
            "O\n",
            "OUSUSUSUSUSUSUSUSUSUSUSUSUSUSUSUSUSUSUSUSUSUSUSUSUSUSUSUSUSUSUSUSUSUSUSUSUSUSUSUSUSUSUSUSUSUSUSUSUSUSUSUSUSUSUSUSUSUSUSUSUSUSUSUSUSUSUSUSUSUSUSUSUSUSUSUSUSUSUSUSUSUSUSUSUSUSUSUSUSUSUSUSUSUSUSUSUSUSUSUSUSUSUSUSUSUSUSUSUSUSUSUSUSUSUSUSUSUSUSUSUSUSUSUSUSUSUSUSUSUSUSUSUSUSUSUSUSUSUSUS\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**3- BLEU Score**\n",
        "\n",
        "TODO: Measures similarity between generated and real text for each model and leave the results in the Notebook."
      ],
      "metadata": {
        "id": "pPFZTX_INVsH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def compute_bleu(reference, generated):\n",
        "    return sentence_bleu([list(reference)], list(generated))\n",
        "\n",
        "def evaluate_model_bleu(model, dataset):\n",
        "    bleu_scores = []\n",
        "\n",
        "    for input_batch, label_batch in dataset:\n",
        "        logits = model(input_batch, training=False)\n",
        "        predictions = tf.argmax(logits, axis=-1)\n",
        "\n",
        "        label_batch = label_batch.numpy()\n",
        "\n",
        "        for pred_seq, true_seq in zip(predictions, label_batch):\n",
        "            # Decode sequences to text\n",
        "            pred_text = text_from_ids(pred_seq, chars_from_ids).numpy().decode('utf-8')\n",
        "            true_text = text_from_ids(true_seq, chars_from_ids).numpy().decode('utf-8')\n",
        "\n",
        "            # Compute BLEU score for each sample\n",
        "            bleu = compute_bleu(true_text, pred_text)\n",
        "            bleu_scores.append(bleu)\n",
        "\n",
        "    return np.mean(bleu_scores)"
      ],
      "metadata": {
        "id": "FSUisLXDM3Is"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluate Perplexity\n",
        "perp = evaluate_model_perplexity(model, validation_dataset)\n",
        "print(f\"LSTM Perplexity on validation dataset: {perp}\")\n",
        "\n",
        "# Evaluate BLEU Score\n",
        "bleu = evaluate_model_bleu(model, validation_dataset)\n",
        "print(f\"LSTM BLEU score on validation dataset: {bleu}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NOPfpa29I7cY",
        "outputId": "30b1f9ac-a77d-4325-80ce-8aec5db8ea0d"
      },
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "LSTM Perplexity on validation dataset: 1.000149953593842\n",
            "LSTM BLEU score on validation dataset: 0.4675834615345109\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluate Perplexity\n",
        "perp = evaluate_model_perplexity(model, validation_dataset)\n",
        "print(f\"GRU Perplexity on validation dataset: {perp}\")\n",
        "\n",
        "# Evaluate BLEU Score\n",
        "bleu = evaluate_model_bleu(model, validation_dataset)\n",
        "print(f\"GRU BLEU score on validation dataset: {bleu}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pMPXd0hOJgd3",
        "outputId": "9823df4d-5cea-4190-b3a6-ef6baba66046"
      },
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GRU Perplexity on validation dataset: 1.0001107370019666\n",
            "GRU BLEU score on validation dataset: 0.6209763079366689\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluate Perplexity\n",
        "perp = evaluate_model_perplexity(model, validation_dataset)\n",
        "print(f\"BiGRU Perplexity on validation dataset: {perp}\")\n",
        "\n",
        "# Evaluate BLEU Score\n",
        "bleu = evaluate_model_bleu(model, validation_dataset)\n",
        "print(f\"BiGRU BLEU score on validation dataset: {bleu}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mmFbiYB-rFsR",
        "outputId": "d0fd0349-b49e-4a0d-cb68-0a61662a508a"
      },
      "execution_count": 69,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "BiGRU Perplexity on validation dataset: 1.000002015669248\n",
            "BiGRU BLEU score on validation dataset: 0.9965349018650352\n"
          ]
        }
      ]
    }
  ]
}